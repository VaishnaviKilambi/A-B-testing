{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b4cf90",
   "metadata": {},
   "source": [
    "#Designing Randomized Control Experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d2669",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "\n",
    "The focus of this lab is on choosing the size or length of an experiment. This stage of experiment design is called a **power analysis**. In determining the scale of an experiment, we face a key tradeoff: larger samples give us more precise and definitive answers, which help us make decisions, but at some cost (e.g. opportunity cost, time).\n",
    "\n",
    "In this lab, rather than design an experiment from scratch, we will take an existing experiment--a pricing experiment conducted by the job matching platform ZipRecruiter--and consider how things might have gone differently if ZipRecruiter had made different choices when designing the experiment. This will illustrate the importance of carefully considering the tradeoffs associated with experiment scale up front.\n",
    "\n",
    "**This week:** We discuss experiment design under practical constraints, including situations where it is difficult to force people into or exclude people from the treatment and how to determine the size or duration of an experiment.\n",
    "\n",
    "**Next week:** We will talk about what to do when we cannot or have not run any experiment at all. We will discuss how to identify `natural' quasi-experiments that can approximate the virtues of an RCT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eebca1",
   "metadata": {},
   "source": [
    "## Background: A Pricing Experiment at ZipRecruiter\n",
    "---\n",
    "\n",
    "In August 2015, ZipRecruiter was the fastest-growing HR company in the United States. The platform matched recruiters with qualified prospective employees using a data-driven algorithm. Job applicants could upload their resumés and apply for jobs for free. Business customers paid a monthly fee to gain access to the resumés of matched applicants. Over 40,000 registered companies posted jobs on the platform per month.\n",
    "\n",
    "Ziprecruiter’s largest business segment consisted of “starter” firms, small companies with fewer than 5 employees. This segment represented nearly 50% of its business customer base. Encouraged by its growth, in early 2015, Ziprecruiter raised its base monthly price in the starter segment to \\\\$99 per month. As both the customer base and revenues continued to grow, Ziprecruiter took more interest in its pricing.\n",
    "\n",
    "In early August 2015, the Ziprecruiter management team met with two marketing professors from the University of Chicago Booth School of Business, Jean-Pierre Dubé and Sanjog Misra, to discuss pricing. They designed and implemented a pricing experiment focused exclusively on new businesses in the starter segment that were reaching Ziprecruiter’s paywall for the first time.\n",
    "\n",
    "In order to obtain a price quote from Ziprecruiter, a firm must first register for Ziprecuiter’s services by navigating a series of pages on the ziprecruiter.com website until it reaches the paywall. At the paywall, the new firm is issued a price quote. It must then use a credit card to pay the subscription fee in order to gain access to Ziprecruiter’s services and network of applicants and resumes.\n",
    "\n",
    "The experiment was conducted between August 28, 2015 and September 29, 2015. During this period, 7,867 unique prospective starter customers reached Ziprecruiter’s paywall. Each prospective customer was randomly assigned to one of ten prices, ranging from \\\\$19 to \\\\$399 per month. One of the price points included in the experiment was Ziprecruiter’s standard \\\\$99  monthly rate, which we can think of as the control group. There are about 800 customers quoted each price.\n",
    "\n",
    "In this lab we will analyze data from this experiment. We will also consider the following thought experiment: what would have happened if Ziprecruiter had run a smaller experiment with fewer customers and fewer price options? This thought experiment will clarify the importance of power analyses and thinking strategically when designing an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76453631",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Opening the data](#openingdata) <br>\n",
    "2. [Analyzing the experiment](#analyzingexperiment) <br>\n",
    "3. [What if ZipRecruiter had run a smaller experiment?](#whatif) <br>\n",
    "4. [Conducting a power analysis?](#poweranalysis) <br>\n",
    "5. [Introducing the `power.t.test` function](#function) <br>\n",
    "6. [Power calculations for a range of sample sizes](#powerbysample) <br>\n",
    "7. [Power calculations for a range of effect sizes](#powerbyeffect) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6bb49",
   "metadata": {},
   "source": [
    "### Opening the data  <a id='openingdata'></a>\n",
    "\n",
    "We will use data on prospective starter customers that were included in the experiment. Each row represents a prospective customer. The data include three columns: `price`, `paid`, and `revenue`.\n",
    "\n",
    "`price`: the price offered to the customer\n",
    "\n",
    "`paid`: an indicator for whether the customer signed up for the service\n",
    "\n",
    "`revenue`: monthly revenue associated with the customer (`revenue` = `price`*`paid`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e63b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "\n",
    "library(dplyr)\n",
    "library(purrr)\n",
    "library(ggplot2)\n",
    "library(estimatr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "ZipRecruiter <- read.csv(\"Ziprecruiter.csv\")\n",
    "\n",
    "head(ZipRecruiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd01035",
   "metadata": {},
   "source": [
    "### Analyzing the experiment <a id='analyzingexperiment'></a>\n",
    "\n",
    "Let's begin by evaluating the original experiment. There are two outcomes we will look at: conversion rates (`paid`) and revenue per prospective customer (`revenue`). We can evaluate the experiment using a regression model, where we regress the outcome on indicator variables for each value of `price`.\n",
    "\n",
    "We will start by looking at the conversion rate.\n",
    "\n",
    "   <span style=\"color:red\">**Warning:** </span> we must write \"`as.factor(price)`\" not \"`price`\" to get indicator variables for each value of `price`.\n",
    "- as.factor(`price`) is interpreted by R as a series of indicator variables for each price level. We will use the `ref` argument to set the reference level (i.e., the omitted price).\n",
    "- Just `price` would be interpreted by R as the original continuous variable, the price faced by the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression model for conversion rates\n",
    "#note: set price = 99 as reference level\n",
    "conversion_model <-\n",
    "summary(conversion_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b331476",
   "metadata": {},
   "source": [
    "Of course, conversion rates are decreasing in price. But conversion rates are perhaps surprisingly high even at the highest price point.\n",
    "\n",
    "It is perhaps easier to see the results in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store summary statistics by price from the experiment\n",
    "results <- ZipRecruiter %>% \n",
    "    group_by(price) %>%\n",
    "    summarize(price = mean(price),\n",
    "              rate = mean(paid),\n",
    "              rate_se = sd(paid)/sqrt(n()),\n",
    "              avg_revenue = mean(revenue),\n",
    "              revenue_se = sd(revenue)/sqrt(n())) %>%\n",
    "    mutate(control = ifelse(price == 99, 1, 0))\n",
    "\n",
    "#what does this new table look like?\n",
    "head(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot conversion rates by price\n",
    "ggplot(results, aes(x = price, y = rate, fill = as.factor(control))) +\n",
    "    geom_bar(stat = \"identity\", position = \"dodge\") +\n",
    "    geom_errorbar(aes(ymin = rate - 1.96*rate_se, ymax = rate + 1.96*rate_se), width = 10) +\n",
    "  xlab('Price') + ylab(\"Conversion Rate\") + theme(legend.position=\"none\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862eb1a0",
   "metadata": {},
   "source": [
    "Given that conversion rates don't tank completely at higher price levels, ZipRecruiter may actually make more money by charging a higher price than \\\\$99. Let's look at a similar regression for revenue per lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35466197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression model for revenue per lead\n",
    "#note: set price = 99 as reference level\n",
    "revenue_model <- \n",
    "\n",
    "summary(revenue_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7667b",
   "metadata": {},
   "source": [
    "Again we can look at the results in bar chart form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot revenue per lead by price\n",
    "ggplot(results, aes(x = price, y = avg_revenue, fill = as.factor(control))) +\n",
    "    geom_bar(stat = \"identity\", position = \"dodge\") +\n",
    "    geom_errorbar(aes(ymin = avg_revenue - 1.96*revenue_se, ymax = avg_revenue + 1.96*revenue_se), width = 10) +\n",
    "  xlab('Price') + ylab(\"Revenue per Lead\") + theme(legend.position=\"none\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5057ea",
   "metadata": {},
   "source": [
    "Despite the fact that conversion rates are decreasing in price, revenue per lead is actually *increasing* in price, at least up to \\\\$249 or so. That much is clear, even given the uncertainty reflected in confidence intervals. \n",
    "\n",
    "**ZipRecruiter ultimately chooses \\\\$249 as their price 3 days after the experiment ends.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48dd61b",
   "metadata": {},
   "source": [
    "### What if ZipRecruiter had run a smaller experiment? <a id='whatif'></a>\n",
    "\n",
    "Consider this thought experiment: what if ZipRecruiter had decided to conduct a smaller experiment? What would they have concluded if they had taken a smaller sample and just looked at two prices, say \\\\$99 and \\\\$249? Would they have come to the same conclusion and increased their price? Or might they made a mistake and come to a different conclusion?\n",
    "\n",
    "Suppose that only 20\\% as many customers were assigned to prices of \\\\$99 and \\\\$249. That's about 160 (= 0.2 * 800) customers in each group.\n",
    "\n",
    "The cell below draws a random sample of observations from the original experiment with 20% as many observations as the original experiment. You can run the cell below a few times to see how your answer changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what are results using 20% sample?\n",
    "sample <- ZipRecruiter %>%\n",
    "filter(...) %>%\n",
    "slice_sample(prop = 0.20, replace = TRUE)\n",
    "\n",
    "#re-estimate regression using sample\n",
    "sample_model <- \n",
    "\n",
    "summary(sample_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd18367",
   "metadata": {},
   "source": [
    "You should see that, from iteration and iteration, the treatment effect estimate can vary quite a bit. And certainly the t-statistic is smaller. In some samples, we may not even get a statistically significant effect. This is the cost of running a smaller experiment--more noise in our estimates.\n",
    "\n",
    "To look at this pattern more systematically, let's draw 1,000 random samples and look at how much t-statistics can vary and how often we would (correctly) reject the null hypothesis that prices of \\\\$99 and \\\\$249 lead to the same revenue per lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame to store estimates\n",
    "estimates = data.frame()\n",
    "\n",
    "#loop through the following code 1000 times\n",
    "for (i in 1:1000) {\n",
    "\n",
    "    #draw sample\n",
    "    sample <- \n",
    "\n",
    "    #estimate model\n",
    "    sample_model <- \n",
    "\n",
    "    #store coefficients\n",
    "    output = c(summary(sample_model)$coefficients[2,1], summary(model)$coefficients[2,3])\n",
    "\n",
    "    #append coefficients to data frame\n",
    "    estimates = rbind(estimates, output)\n",
    "}\n",
    "\n",
    "#label column names\n",
    "colnames(estimates)<-c(\"estimate\", \"tstat\")\n",
    "\n",
    "#what does the end result look like?\n",
    "head(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccfa338",
   "metadata": {},
   "source": [
    "For each new sample we draw, we run another hypothesis and get another t-statistic. Here's what the distribution of those t-statistics looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of t-statistics\n",
    "ggplot(estimates, aes(x=tstat)) + geom_histogram() + \n",
    "    theme_classic() + ggtitle(\"Histogram of t-Statistics\") + \n",
    "    geom_vline(xintercept = 1.96, color=\"red\") + \n",
    "    xlab(\"t-Stat\") + ylab(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698c8c3",
   "metadata": {},
   "source": [
    "Finally, let's calculate how often we (correctly) reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e03003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate null rejection rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb6f38",
   "metadata": {},
   "source": [
    "We would only correctly reject the null about 60% of the time. That means that, if we had run this smaller experiment instead, we would have drawn the wrong conclusion 40% of the time and left a sizeable amount of revenue on the table.\n",
    "\n",
    "\n",
    "What would happen if we had chosen another price that was even closer to status quo price, \\\\$99? You can see this by looking at the results for \\\\$79. They are essentially indistinguishable from the conversion rates and revenue per prospective customer for \\\\$99, even using all the experiment data. If we had just run an experiment with \\\\$99 and \\\\$79, we would have needed a bigger experiment to determine whether that was a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e874683",
   "metadata": {},
   "source": [
    "### Conducting a power analysis <a id='poweranalysis'></a>\n",
    "\n",
    "Larger sample sizes mean more precise estimates of treatment effects.\n",
    "\n",
    "- But it can take a lot of resources to run a large experiment.\n",
    "\n",
    "What sample size should we choose? We can answer this question with *power calculations*.\n",
    "\n",
    "The *power* of a statistical test is **the probability of finding a statistically significant difference** for a given effect size. \n",
    "\n",
    "We often measure effect size in **standard deviations**. Let's calculate the standard deviation of `revenue` in the experiment when the price is \\\\$99 or \\\\$249.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate standard deviation of `revenue` if price is $99 or $249"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61289845",
   "metadata": {},
   "source": [
    "   - The standard deviation of our outcomes is about **\\\\$72**.\n",
    "  \n",
    "   - We know from the original experiment that charging \\\\$249 increases revenue per lead by about **\\\\$18.**\n",
    "  \n",
    "   - Thus the effect size in standard deviations (sd) is 18/72 = **0.25 SD**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c3616",
   "metadata": {},
   "source": [
    "### Introducing the `power.t.test` function <a id='function'></a>\n",
    "    \n",
    "How powered was our n = 320 (160 per group) experiment to detect a 0.25 SD effect at the 0.05 significance level? \n",
    "    \n",
    "  - We will use the `power.t.test` function for our power calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ba304",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Arguments:\n",
    "  # delta = effect size (sd=1 means the outcome is in standard deviations).\n",
    "    # our effect size is 0.3.\n",
    "  # sig.level is significance level: we choose a 0.05 significance threshold.\n",
    "  # n is our sample size (per group).\n",
    "\n",
    "power.t.test(delta=...,sd=...,sig.level=...,n=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da8d68",
   "metadata": {},
   "source": [
    "Our 320-customer sample was only powered at the 0.61 level. This is close to what we got in the simulation above.\n",
    "\n",
    "- This means that a significant relationship would only be detected 61% of the time. The other 29% would be **false negatives**.\n",
    "\n",
    "What about the original 800 per group experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33535ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "power.t.test(delta=...,sd=...,sig.level=...,n =...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e5320",
   "metadata": {},
   "source": [
    "Now our power is 0.9988. This is potentially overkill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793b707",
   "metadata": {},
   "source": [
    "### Power calculations for a range of sample sizes <a id='powerbysample'></a>\n",
    "\n",
    "So far:\n",
    " - We calculated the **power** given an effect size, significance level, and a sample size. \n",
    "\n",
    "But `power.t.test` is more flexible than that. If you feed `power.t.test` **three** of the following inputs, and the `power.t.test` will give you the fourth.\n",
    "\n",
    " - **Sample size**: number of observations per experimental group\n",
    " - **Significance level** (we typically choose p < 0.05 or 0.01)\n",
    " - **Power** (we typically choose power = 0.8 or 0.9)\n",
    " - **Effect size**. Setting SD = 1 means the effect size will be measured in standard deviations\n",
    "     \n",
    "For example: \n",
    "- **Input** significance level, power, and effect size. `power.t.test` outputs **sample size**.\n",
    "\n",
    "- **Input** significance level, sample size, effect size. `power.t.test` outputs **power**.\n",
    "\n",
    "It's often useful to look at the power across a range of sample sizes.\n",
    "\n",
    "- As an example, let's consider samples sizes per price ranging from 100 to 3000 prospective customers for an effect size of 0.1 standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e832b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code generates a list from 100 to 3000 counting by 50.\n",
    "samplesizes <- seq(from=100,to=3000,by=50)\n",
    "\n",
    "# Now we calculate power for each value on the list.\n",
    "power.samplesizes <- power.t.test(n=samplesizes,delta=0.1,sd=1,sig.level=0.05,type=\"two.sample\")$power\n",
    "\n",
    "# And plot the results:\n",
    "plot(samplesizes,\n",
    "     power.samplesizes,\n",
    "     xlim=c(0,3000),\n",
    "     xlab=\"Sample size\",\n",
    "     ylab=\"Expected power: 0.1 sd effect\",\n",
    "     ylim=c(0,1),\n",
    "     col=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a9098",
   "metadata": {},
   "source": [
    "So, if we want to achieve power of 0.8 or more, we will need at least ~1600 participants per group!\n",
    "\n",
    "What if we thought the effect size was **0.3 standard deviations**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "power.samplesizes <- power.t.test(n=...,delta=...,sd=...,sig.level=...,type=\"two.sample\")$power\n",
    "\n",
    "plot(samplesizes,\n",
    "     power.samplesizes,\n",
    "     xlim=c(0,3000),\n",
    "     xlab=\"Sample size\",\n",
    "     ylab=\"Expected power: 0.3 sd effect\",\n",
    "     ylim=c(0,1),\n",
    "     col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a050829",
   "metadata": {},
   "source": [
    "We would need only ~200 people per group to achieve our desired statistical power!\n",
    "\n",
    "**Challenge**: What effect size do we use?\n",
    "\n",
    "(Note: if we already knew this, we wouldn't need to run an experiment!)\n",
    "\n",
    "- Use smallest effect that would pass cost-benefit assessment or some other benchmark\n",
    "- Or: use informed guess based on context or prior findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6892bf",
   "metadata": {},
   "source": [
    "### Power calculations for a range of effect sizes <a id='powerbyeffect'></a>\n",
    "\n",
    "We can also think about what effect size we would be able to reliably detect for a given sample size. This is relevant for when you're deciding what is worth testing in the first place. For a relatively small experiment, a subtle intervention or small tweak may have such a small expected effect size that it's not worth testing.\n",
    "\n",
    "Here we'll set a sample size of 800 per group. We'll consider effect sizes ranging from 0.01 to 0.3 SDs in increments of 0.01 SDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code generates a list from 0.01 to 0.3 counting by 0.01.\n",
    "effectsizes <- seq(from=0.01,to=0.3,by=0.01)\n",
    "\n",
    "# Now we calculate power for each effect size value on the list.\n",
    "power.effectsizes <- power.t.test(n= ...,delta=...,sd=...,sig.level=...,type=\"two.sample\")$power\n",
    "\n",
    "# And plot the results:\n",
    "plot(effectsizes,\n",
    "     power.effectsizes,\n",
    "     xlim=c(0,0.3),\n",
    "     xlab=\"Effect size\",\n",
    "     ylab=\"Expected power: sample of 800\",\n",
    "     ylim=c(0,1),\n",
    "     col=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7990d",
   "metadata": {},
   "source": [
    "For a sample size of 800 per group, we should expect an effect size of about 0.15 SDs to have power greater than 0.8. For this sample size, we should reconsider testing interventions where we would need to be able to detect smaller effects than 0.15 SDs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
